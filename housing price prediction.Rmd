---
title: "Predict the Housing Prices in Ames"
output: html_notebook
---
load data
```{r}
## load data
library(caret)
library(ggplot2)
library(corrplot)
library(e1071)
#setwd("/Users/Morello/Desktop/2016 SPRING/STAT 542/project 1")
train_data <- read.csv('train.csv',header = FALSE,quote = "")
names(train_data) <- as.matrix(train_data[1, ])
train_data <- train_data[-1, ]
train_data[] <- lapply(train_data, function(x) type.convert(as.character(x)))
test_data <- read.csv('test.csv',header = FALSE)
names(test_data) <- as.matrix(test_data[1, ])
test_data <- test_data[-1, ]
test_data[] <- lapply(test_data, function(x) type.convert(as.character(x)))
```

```{r}
numNA = colSums(apply(train_data[, -c(1, 81)], 2, is.na))
number_of_missing = numNA[which(numNA != 0)]  # number of NA's
data_type = sapply(train_data[,names(which(numNA != 0))], class)  # type of data
## cbind(number_of_missing, data_type)
drops = c("Alley", "PoolQC", "Fence", "MiscFeature")
train = train_data[ , !(names(train_data) %in% drops)]
test = test_data[ , !(names(test_data) %in% drops)]
data.type = sapply(train[, -c(1, ncol(train))], class)
cat_var = names(train)[which(c(NA, data.type, NA) == 'factor')]  # categorical variables
numeric_var =  names(train)[which(c(NA, data.type, NA) == 'integer')]  # continuous variables
for (j in cat_var){
  train[, j] = addNA(train[, j])  # addNA treat the NA's as a new level called '<NA>'
  test[, j] = addNA(test[, j])
}
tempVar = c('LotFrontage', 'MasVnrArea', 'GarageYrBlt')
for (j in tempVar){
  na.id = is.na(train[, j])  # binary indicator: NA (1) or not (0)
  tempMedian = median(train[, j], na.rm = TRUE)  # find the median
  train[which(na.id), j] = tempMedian
  train[, paste(j, 'NAInd', sep = '_')] = as.numeric(na.id)  # create a new column
}
for (j in numeric_var){
  na.id = is.na(test[, j])
  if (!any(na.id)){
    next
  }
  test[which(na.id), j] = median(train[, j])
}
# transform SalePrice target to log form
train$SalePrice <- log(train$SalePrice + 1)

# for numeric feature with excessive skewness, perform log transformation
# determine skew for each numeric feature
skewed_feats = sapply(train[, numeric_var], skewness)
# only transform features that exceed a threshold = 0.75 for skewness
skewed_feats = numeric_var[which(skewed_feats > 0.75)]
for(j in skewed_feats) {
  train[, j] = log(train[, j] + 1)
}
```

```{r}
summary(train)
```

```{r}
# split train data and price
train_price <- train[,77]
train_data1 <- train[,-c(77)]
## split data into three part: training part; cv part; test part;
datasplit1 <- createDataPartition( y = train_price, p=.75, list = FALSE)
train_x <- train_data1[datasplit1,]
train_y <- train_price[datasplit1]
datasplit2 <- createDataPartition( y = train_y, p=.8, list = FALSE)
cv_x <- train_x[-datasplit2,]
cv_y <- train_y[-datasplit2]
train_x1 <- train_x[datasplit2,]
train_y1 <- train_y[datasplit2]
test_x <- train_data1[-datasplit1,]
test_y <- train_price[-datasplit1]
```

```{r}
library(pls)
pcr_model1 <- pcr(train_y~., data = train_x, validation = "CV", ncomp =20)
summary(pcr_model1)
validationplot(pcr_model1)
validationplot(pcr_model1, val.type = "R2")
validationplot(pcr_model1, val.type="MSEP")
coefplot(pcr_model1)
pred1 <- predict(pcr_model1, test_x, ncomp = 20)
pls_model2 <- plsr(train_y~., data = train_x, validation = "CV", ncomp =20)
pred2 <- predict(pls_model2, test_x)
```


```{r}
result1 <- exp(pred1) + 1 
result2 <- exp(pred2) + 1
error1 <- sqrt(mean((pred1 - test_y)^2))
error2 <- sqrt(mean((pred2 - test_y)^2))
error1
error2
#submission = read.csv('./input/sample_submission.csv')
#submission$SalePrice = exp(pred1)-1
#write.table(mysubmission1, 'simple_model.csv', row.names = FALSE, sep = ',')
```

```{r}
library(FactoMineR)
afdm_model <- FAMD(train_x, ncp = 140)
summary(afdm_model)
```


For Linear Regression
Calculate the correlations and then select columns that most correlate to the SalePrice
```{r}
correlations = cor(train[, c(numeric_var, 'SalePrice')])  # correlation matrix
# for those relatively large correlations (> 0.3)
row_indic = apply(correlations, 1, function(x) sum(abs(x) > 0.3) > 1)
correlations = correlations[row_indic, row_indic]
corrplot(correlations, method = "square")
highCor = which(abs(correlations[, ncol(correlations)]) > 0.5)
highCor = highCor[-length(highCor)]
names(highCor)
par(mfrow = c(2,3))
for (j in 1:length(highCor)){
  plot(train[, names(highCor)[j]], train[, 'SalePrice'], xlab = names(highCor)[j], ylab = 'SalePrice')
  tempModel = lm(SalePrice ~ ., data = train[, c(names(highCor)[j], 'SalePrice')])
  abline(tempModel, col = 'blue')
  legend('topleft', legend = paste('cor = ', round(correlations[highCor[j], ncol(correlations)], 3)))
}
```

```{r}
new_train <- subset(train_x, select = c("OverallQual", "YearBuilt", "YearRemodAdd", "1stFlrSF","GrLivArea", "FullBath",     "TotRmsAbvGrd", "GarageCars", "GarageArea" ))
new_test <- subset(test_x, select = c("OverallQual", "YearBuilt", "YearRemodAdd", "1stFlrSF","GrLivArea", "FullBath",     "TotRmsAbvGrd", "GarageCars", "GarageArea" ))
new_train <- model.matrix(~ .-1, new_train)
new_test <- model.matrix(~ .-1, new_test)
#full = lm(train_y ~., data = new_train);  
#Ytest.pred = predict(full, newdata= new_test);
#sqrt(mean((test_y - Ytest.pred)^2))
```

Ridge
```{r}
library(MASS)
library(glmnet)
lam.seq = 10^seq(10, -2, length=100)
myridge = glmnet(new_train, train_y, alpha=0, lambda = lam.seq)
plot(myridge)
names(myridge)
dim(coef(myridge))      # retrieve the coefficients
length(myridge$lambda)  # retrieve the lambda value
cv.out = cv.glmnet(new_train, train_y, alpha=0)  # lambda sequence set by glmnet
plot(cv.out)
names(cv.out)
cv.out$lambda[which.min(cv.out$cvm)]
cv.out$lambda.min
tmp.id=which.min(cv.out$cvm)
max(cv.out$lambda[cv.out$cvm < cv.out$cvm[tmp.id] + cv.out$cvsd[tmp.id]])
cv.out$lambda.1se
lam.seq = exp(seq(-6, 2, length=100))
cv.out = cv.glmnet(new_train, train_y, alpha=0, lambda=lam.seq)  
plot(cv.out)
myridge = glmnet(new_train,train_y, alpha=0, lambda=lam.seq)
bestlam.ridge = cv.out$lambda.min
Ytest.pred=predict(myridge,s=bestlam.ridge ,newx=new_test)
```

```{r}
sqrt(mean((Ytest.pred - test_y)^2))
```

Lasso
```{r}
library(glmnet)
mylasso = glmnet(new_train, train_y, alpha=1)
plot(mylasso)
cv.out = cv.glmnet(new_train, train_y, alpha=1)
plot(cv.out)
lam.seq =  exp(seq(-4, 2, length=100))
cv.out = cv.glmnet(new_train, train_y, alpha=1, lambda =lam.seq)
plot(cv.out)
cv.out$lambda.min
tmp.id=which.min(cv.out$cvm)
cv.out$lambda[tmp.id]
cv.out$lambda.1se
max(cv.out$lambda[cv.out$cvm < cv.out$cvm[tmp.id] + cv.out$cvsd[tmp.id]])
mylasso = glmnet(new_train, train_y, alpha=1, lambda =lam.seq)
bestlam.lasso = cv.out$lambda.min
Ytest.pred=predict(mylasso, s=bestlam.lasso ,newx=new_test)
```

```{r}
sqrt(mean((Ytest.pred - test_y)^2))
```
```{r}
mylasso.coef = predict(mylasso, s=bestlam.lasso, type="coefficients")
mylasso.coef
bestlam.lasso = cv.out$lambda.1se
Ytest.pred=predict(mylasso, s=bestlam.lasso ,newx=new_test)
```
```{r}
sqrt(mean((Ytest.pred - test_y)^2))
```
Full Model
```{r}
library(stats)
highCor1 = which(abs(correlations[, ncol(correlations)]) > 0)
highCor1
highCor1 = highCor1[-length(highCor1)]
highCor1
names(highCor1)
full.model = lm(train_y ~., data = train_x[,c(names(highCor1))]);  
Ytest.pred = predict(full.model, newdata= model.frame(test_x));
sqrt(mean((test_y - Ytest.pred)^2)) # averaged MSE on the test set
```
```{r}
stepAIC = step(full.model, trace=0, direction="both")            
AIC.model = eval(stepAIC$call)
Ytest.pred = predict(AIC.model, newdata= test_x);
sqrt(mean((test_y - Ytest.pred)^2))

sel.var=attr(stepAIC$terms, "term.labels") # retrieve the list of selected variables
sel.var
length(sel.var)                            # size of the selected model     

stepBIC = step(full.model, trace=0, direction="both")            
BIC.model = eval(stepBIC$call)
Ytest.pred = predict(BIC.model, newdata = test_x);
sqrt(mean((test_y - Ytest.pred)^2))

sel.var=attr(stepBIC$terms, "term.labels") # retrieve the list of selected variables
sel.var
length(sel.var)

#submission = read.csv('./input/sample_submission.csv')
#submission$SalePrice = exp(Ytest.pred)-1
#write.table(mysubmission2, 'simple_model.csv', row.names = FALSE, sep = ',')
```
#### R, gbm(), distribution "laplace" ####
```{r}
library(dplyr)
library(tidyr)
library(rpart)
library(randomForest)
library(ggplot2)
library(gbm)
start=proc.time()
model <- gbm(train_y ~., data = train_x, distribution = "laplace",
              shrinkage = 0.05,
              interaction.depth = 5,
              bag.fraction = 0.66,
              n.minobsinnode = 1,
              cv.folds = 100,
              keep.data = F,
              verbose = F,
              n.trees = 300)
GBM <- predict(model, test_x, n.trees = 300)
proc.time()-start

# RMSE
sqrt(mean((GBM - test_y)^2))

#submission = read.csv('./input/sample_submission.csv')
#submission$SalePrice = exp(GBM)-1
#write.table(mysubmission3, 'simple_model.csv', row.names = FALSE, sep = ',')
```